{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9441fa4d-632e-430b-bbd0-b3db7ba54c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ebf3e4-3c0a-4ee8-a6a4-6d7e3726e496",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6af3895-2035-4a74-99e8-7f83d4b550de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets optuna transformers -U colorlog evaluate huggingface_hub multiprocess xxhash regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2551dba-bdb9-407d-a1a8-00cb3d1093be",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493692c0-4e5d-4b4a-857c-b3cb031639aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export TF_USE_LEGACY_KERAS=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b77a13-fb08-42d6-9aa1-a2447bae43d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e0eb29-89f6-4950-93a8-83449a15e050",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4967db9-3b83-46c7-ac85-becc06b5e22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import ViTImageProcessor, AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomResizedCrop,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    ")\n",
    "import numpy as np\n",
    "import torch\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bb5e0a-372a-402f-9e3a-a0f733a2e417",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"/home/jovyan/datafabric/fd_hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d307a6b-86da-4cfa-a479-c854ea520c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \n",
    "    print(dataset[\"train\"].features)\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "    labels = dataset[\"train\"].features[\"label\"].names\n",
    "    label2id, id2label = dict(), dict()\n",
    "    for i, label in enumerate(labels):\n",
    "        label2id[label] = i\n",
    "        id2label[i] = label\n",
    "        \n",
    "    model_checkpoint = \"google/vit-base-patch16-224\"\n",
    "    batch_size = 16\n",
    "        \n",
    "    image_processor  = ViTImageProcessor.from_pretrained(model_checkpoint)\n",
    "\n",
    "    normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "    if \"height\" in image_processor.size:\n",
    "        size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    "        crop_size = size\n",
    "        max_size = None\n",
    "    elif \"shortest_edge\" in image_processor.size:\n",
    "        size = image_processor.size[\"shortest_edge\"]\n",
    "        crop_size = (size, size)\n",
    "        max_size = image_processor.size.get(\"longest_edge\")\n",
    "\n",
    "    train_transforms = Compose(\n",
    "            [\n",
    "                RandomResizedCrop(crop_size),\n",
    "                RandomHorizontalFlip(),\n",
    "                ToTensor(),\n",
    "                normalize,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    val_transforms = Compose(\n",
    "            [\n",
    "                Resize(size),\n",
    "                CenterCrop(crop_size),\n",
    "                ToTensor(),\n",
    "                normalize,\n",
    "            ]\n",
    "        )\n",
    "    def preprocess_train(example_batch):\n",
    "        \"\"\"Apply train_transforms across a batch.\"\"\"\n",
    "        example_batch[\"pixel_values\"] = [\n",
    "            train_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]\n",
    "        ]\n",
    "        del example_batch[\"image\"]  # Remove the original image field\n",
    "        return example_batch\n",
    "\n",
    "    def preprocess_val(example_batch):\n",
    "        \"\"\"Apply val_transforms across a batch.\"\"\"\n",
    "        example_batch[\"pixel_values\"] = [val_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n",
    "        del example_batch[\"image\"]  # Remove the original image field\n",
    "        return example_batch\n",
    "\n",
    "    train_ds = dataset['train']\n",
    "    val_ds = dataset['test']\n",
    "\n",
    "        # Apply the preprocessing\n",
    "    train_ds = train_ds.with_transform(preprocess_train)\n",
    "    val_ds = val_ds.with_transform(preprocess_val)\n",
    "    \n",
    "    model = AutoModelForImageClassification.from_pretrained(\"google/vit-base-patch16-224\", ignore_mismatched_sizes=True, label2id=label2id,\n",
    "        id2label=id2label,)\n",
    "\n",
    "    model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    " \n",
    "    def compute_metrics(eval_pred):\n",
    "        \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
    "        predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "        return metric.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "\n",
    "\n",
    "\n",
    "    # Suggest hyperparameters\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n",
    "    num_epochs = trial.suggest_int(\"num_epochs\", 1, 5)\n",
    "    gradient_accumulation_steps = trial.suggest_int(\"gradient_accumulation_steps\", 1, 8)\n",
    "    per_gpu_batch_size = trial.suggest_int(\"per_gpu_batch_size\", 8, 32)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 0, 0.5)\n",
    "    warmup_steps = trial.suggest_int(\"warmup_steps\", 0, 100)\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "         f\"{model_name}-finetuned-forgery\",\n",
    "        learning_rate=learning_rate,\n",
    "        num_train_epochs=num_epochs,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        per_gpu_train_batch_size = per_gpu_batch_size,\n",
    "        weight_decay=weight_decay,\n",
    "        warmup_steps=warmup_steps,\n",
    "        remove_unused_columns=False,\n",
    "        eval_strategy = \"epoch\",\n",
    "        save_strategy = \"epoch\",\n",
    "        logging_steps=10,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_accuracy\",\n",
    "        push_to_hub=False,\n",
    "    )\n",
    "    # Train your model\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        tokenizer=image_processor,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    train_results = trainer.train()\n",
    "    # Evaluate your model\n",
    "    eval_result = trainer.evaluate()\n",
    "    \n",
    "        # rest is optional but nice to have\n",
    "    trainer.save_model('model_checkpoints')\n",
    "    trainer.log_metrics(\"train\", train_results.metrics)\n",
    "    trainer.save_metrics(\"train\", train_results.metrics)\n",
    "    trainer.save_state()\n",
    "\n",
    "    metrics = trainer.evaluate()\n",
    "    # some nice to haves:\n",
    "    trainer.log_metrics(\"eval\", metrics)\n",
    "    trainer.save_metrics(\"eval\", metrics)\n",
    "    \n",
    "    return eval_result[\"eval_accuracy\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c33151d-5916-4914-860f-ce0cad121924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.autolog()\n",
    "# Set up MLflow experiment\n",
    "# mlflow.set_experiment(\"HF Optuna Forgery\")\n",
    "\n",
    "with mlflow.start_run(experiment_id=\"599198156041778570\"):\n",
    "    study = optuna.create_study(direction=\"maximize\")  # maximize accuracy\n",
    "    # Run the optimization\n",
    "    study.optimize(objective, n_trials=1) # Adjust number of trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d08ffab-a317-4b3c-9adc-b9b762e71c84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
